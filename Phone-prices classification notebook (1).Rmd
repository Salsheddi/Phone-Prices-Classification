---
title: "predict phone price |data mining project G3"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---
## 1.Problem
Mobile devices have significantly changed how individuals work, socialize, plan, and entertain themselves. There are numerous companies that provide us with phones with varied features; as part of our project, we will compare the prices of phones from different companies depending on features like brand, operating system, storage, and others. We were interested in this dataset because we want to build a classification model that helps people choose the right phone for them taking into consideration the features compared to the price.





## 2.Data mining task
The objective of this dataset is to use data mining techniques such as classification and clustering to develop a predictive model. This model will categorize and estimate the price range of popular phone brands based on their distinctive features. Specifically, the aim is to classify phones into categories such as "expensive," "affordable," or "cheap," based on the phone's price, and clustering while taking into account their ongoing manufacturing and the wealth of information contained in the dataset.

## 3.Data

### -Dataset Source :
We took the dataset from kaggle, an online platfrom for data science competitons and collaboration, offering datasets, computing resources and a community of data scientists

Dataset URL : https://www.kaggle.com/datasets/berkayeserr/phone-prices

### -main characteristics of attributes :
The dataset consists of 22 columns(attributes) and 1512 rows


| Attributes name | Describtion | Data type | possible values |
|-----------------|-------------|-----------|-----------------|
|phone_name       |name of the phone|string|1496 possible unique values|
|brand            |brand of the phone|string|Xiaomi, Oppo, Samsung, Vivo, Realme]|
|os               |operating system of the phone|string|[Android 11, Android 10 ,Android 12, Android 9.0 ,Android 13]|
|inches           |size of the phone screen |numeric |3.5 - 10.5  |
|resoultion       |resoultion of the phone screen width x height|character           |1080x2400 720x1600 1080x2340 1080x2408 720x1520         |
|battery          |battery capacity of the phone|numeric|1500 - 7500| 
|battery_type     |battery type of the phone|character| Li-Po Li-Ion |
|ram(GB)          |ram of the phone as gigabyte|numeric |0 -24                 |
|announcement_date |the date of the announcement of the phone             |character           |1 sep 2016 - 31 Aug 23 |
|weight(g)         |weight of the phone in grams |numeric           |100-500                 |
|storage(GB)      |storage capacity of the phone as GB             |numeric           |0-550                 |
|video_720p       |does phone camera has 720p feature|boolean|TRUE , FALSE                 |
|video_1080p      |does phone camera has 1080p feature       |logical           |TRUE , FALSE                 |
|video_4K         |does phone camera has 4K feature             |logical           |TRUE , FALSE                 |
|video_8K         |does phone camera has 8K feature|logical           |TRUE , FALSE                |
|video_30fps       |does phone camera has 30fps feature             |logical           |TRUE , FLASE                 |
|video_120fps       |does phone camera has 120fps feature             |logical           |TRUE , FALSE                 |
|video_240fps       |does phone camera has 240fps feature             |logical           |TRUE , FALSE                 |
|video_480fps|does phone camera has 480fps feature|logical|TRUE , FALSE|
|video_960fps|does phone camera has 960fps feature|logical|TRUE , FALSE|
|price(USD| price of the phone as USD|numeric |0 - 2400|

## -Data loading
```{r}
dataset_phones=read.csv("cleaned_all_phones.csv",header=TRUE,sep=',') 
```
## -libraries and packages loading
```{r}
install.packages("ggplot2") 
library(ggplot2)
installed.packages("dplyr")
library(dplyr)
```



## -Data Exploration and Summary using R

### -number of objects
```{r}
nrow(dataset_phones)

```


### -number of attributes
```{r}
ncol(dataset_phones)
```

### sample of first 10 rows
```{r}
head(dataset_phones,10)
```
### sample of last 10 rows
```{r}
tail(dataset_phones,10)
```




### -structure of the dataset
```{r}
str(dataset_phones)
```
The dataset contains 1512 rows (observations) and 22 columns (variables or attributes),each row represents a different phone, and each column represents a specific attribute or characteristic of the phone.The dataset includes a mix of different data types:
Character data types (e.g., phone_name, brand, os, resolution, battery_type).
Numeric data types (e.g., inches, battery, ram(GB), weight(g), storage(GB), price(USD)).
Logical data types (e.g., video_720p, video_1080p, video_4K, ...).

The data types used for each attribute are generally appropriate based on the nature of the data. for example, character data types are used for textual or categorical information like phone names and brands, while numeric data types are used for measurements such as battery capacity and price .the dataset have been loaded correctly with the appropriate data types assigned to each attribute 

### -checking for missing values

```{r}
dim(dataset_phones)
dataset_phones=na.omit(dataset_phones)
dim(dataset_phones)
sum(is.na(dataset_phones))

```
We checked for missing values and there were no tuples that had a no recorded value (nulls or NA (none answer)) for all attributes which indicates that all tuples have a recorded value, meaning we do not need to do any processing to clean the data from missing values.

### -checking for duplication
```{r}
num_duplicates <- sum(duplicated(dataset_phones))
cat("Number of duplicate rows:", num_duplicates, "\n")
```
there's no duplicate tuples that requires data cleaning

### -statistical measures

```{r}
summary(dataset_phones)
```
After doing the summary we can predict the distribution of the data 
for each column by comparing the mean and the median .
in  inches,battery  columns we can see that both of them are symmetric because Q1,Q2,Q3 are approximately equal
ram.GB.,weight.g.,storage.GB.,price.USD. columns we can see that all of them right-skewed because Q2 is closer to Q3


also we can get  the range for every column by seeing the Max value and the min value .
for the inches column the we can describe the range as the following (3.800 to 10.400)
for battery column the range as the following (1821 to 7250 )
for ram.GB. column the range as the following (1.000 to 24.000) which is the Biggest Range from all column
for weight.g. column the range as the following (130.0 to 500.0)
for storage.GB. column the range as the following (1.0 to 512.0 )
for price.USD. column the range as the following (40.0 to 2300.0)


## Central Tendency
### Mean
```{r}
mean(dataset_phones$inches)
mean(dataset_phones$battery)
mean(dataset_phones$ram.GB.)
mean(dataset_phones$weight.g.)
mean(dataset_phones$storage.GB.)
mean(dataset_phones$price.USD.)

```
the mean represents the central value of the Data Set ,indicating  the average of all of the number,the mean is sensitive to extreme value. 
This single value for the mean is much easier to interpret compared to staring at all of the rows of raw data we can see that battery column has the highest mean which is (4389.799) on the other hand we can see that the inches column has the lowest mean which is (6.42246) which is slightly close to ram.GB. column (6.683862)


### Median 
```{r}
median(dataset_phones$inches)
median(dataset_phones$battery)
median(dataset_phones$ram.GB.)
median(dataset_phones$weight.g.)
median(dataset_phones$storage.GB.)
median(dataset_phones$price.USD.)
```
The median is a measure of central tendency that represents the middle value of a Data set It provides a measure of central tendency that is not affected by extreme values as much as the mean.
The median is an important to calculate because it gives us an idea of where the center of a data set is located for each column  we can see that the center for battery column is (4500) which is the biggest median of all column  on the other hand inches column is (6.5)which is the smallest median.

### Mode
```{r}

variables <- c(
  "phone_name", "brand", "os", "inches", "resolution", "battery",
  "battery_type", "ram.GB.", "announcement_date", "weight.g.",
  "storage.GB.", "video_720p", "video_1080p", "video_4K",
  "video_8K", "video_30fps", "video_60fps", "video_120fps",
  "video_240fps", "video_480fps", "video_960fps", "price.USD."
)


find_mode <- function(x) {
  tab <- table(x)
  names(tab)[tab == max(tab)]
}


for (variable in variables) {
  cat("Variable:", variable, "\n")
  cat("Mode(s):", find_mode(y[[variable]]), "\n\n")
}

```
The mode is the value that occurs with the highest frequency in a dataset. It represents the most common observation or category.
for our data set all the column have one mode which mean it is unimodal.

### Midrange
```{r}
z<-c(dataset_phones)
max(z$inches)+ min(z$inches)/2
max(z$battery)+ min(z$battery)/2
max(z$ram.GB.)+ min(z$ram.GB.)/2
max(z$weight.g.)+ min(z$weight.g.)/2
max(z$storage.GB.)+ min(z$storage.GB.)/2
max(z$price.USD.)+ min(z$price.USD.)/2
```
The midrange is a simple statistical measure that provides a rough estimate of the center of a data set it provides a simple measure that summarizes the spread of data by considering the range.
when we look at the data we can see that battery column has the highest midrange which is(8160.5)on the other hand inches column has the smallest midrange which is(12.3)
### Variance
```{r}
var(dataset_phones$inches)
var(dataset_phones$battery)
var(dataset_phones$ram.GB.)
var(dataset_phones$weight.g.)
var(dataset_phones$storage.GB.)
var(dataset_phones$price.USD.)
```
Variance is a statistical measure used to quantify the spread or dispersion of a set of data points.
The variance is always a non-negative value. A small variance like the variance for inches column (0.2275701)indicates that the data points are closely clustered around the mean, while a large variance like the variance for battery column suggests that the data points are spread out over a wider range.


### Standard Deviation
```{r}
sd(dataset_phones$inches)
sd(dataset_phones$battery)
sd(dataset_phones$ram.GB.)
sd(dataset_phones$weight.g.)
sd(dataset_phones$storage.GB.)
sd(dataset_phones$price.USD.)

```
The standard deviation is a statistical measure that quantifies the dispersion or variability of a set of data points. It is closely related to the variance but provides a measure of dispersion that is in the same unit as the original data.
A small standard deviation like the standard deviation for inches column(0.4770431) suggests that the data points are closely clustered around the mean,which indicating less variability.On the other hand a large standard deviation like the standard deviation for battery column (784.607)indicates that the data points are spread out over a wider range,which indicating greater variability.

## Dispersion of data
### Range 
```{r}
range(dataset_phones$inches)
range(dataset_phones$battery)
range(dataset_phones$ram.GB.)
range(dataset_phones$weight.g.)
range(dataset_phones$storage.GB.)
range(dataset_phones$price.USD.)
```
The range is a simple statistical measure that represents the difference between the highest and lowest values in a data set. It provides a basic understanding of the spread or variability of the data.
A larger range like the range for the inches column( 3.8 to 10.4) indicates a wider spread and greater variability, while a smaller range like the range for the battery column(1821 to 7250) suggests a narrower spread and less variability.

### Quartiles
```{r}
quantile(dataset_phones$inches,c(0.25,0.50,0.75))
quantile(dataset_phones$battery,c(0.25,0.50,0.75))
quantile(dataset_phones$ram.GB.,c(0.25,0.50,0.75))
quantile(dataset_phones$weight.g.,c(0.25,0.50,0.75))
quantile(dataset_phones$storage.GB.,c(0.25,0.50,0.75))
quantile(dataset_phones$price.USD.,c(0.25,0.50,0.75))
```
The three Quartiles are denoted as Q1 is found by finding the median of the values between the minimum and the median, Q2 simply referred to as the median, and Q3 is found by finding the median of the values between
the median and the maximum.



### Interquartile Range
```{r}
IQR(dataset_phones$inches)
IQR(dataset_phones$battery)
IQR(dataset_phones$ram.GB.)
IQR(dataset_phones$weight.g.)
IQR(dataset_phones$storage.GB.)
IQR(dataset_phones$price.USD.)
```
Interquartile Range It measures the spread of the middle 50% of the data and is useful for identifying outliers.
Outliers are data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. By comparing individual data points to the quartiles and the IQR, you can identify values that lie significantly outside the expected range.

for the inches column the max_outlier>7.22 and the min_outlier< 5.74
for the battery column the max_outlier>6500 and the min_outlier< 2500
for the ram.GB. column the max_outlier>14 and the min_outlier<-2 
for the weight.g. column the max_outlier>230.62 and the min_outlier<141.62
for the storage.GB. column the max_outlier>224 and the min_outlier<-32
for the price.USD. column the max_outlier>730.0037 and the min_outlier<-150.0062


## Data visualization

### Box plots and outliers detecting


```{r}
boxplot(dataset_phones$storage.GB. , data = dataset_phones, xlab = "Storage(GB)", ylab = "Frequency")
```



```{r}
boxplot.stats(dataset_phones$storage.GB)$out
```
The storage(GB) boxplot and statistical result illustrates that the range of the storage values are high (1GB - 512GB)  and there’s outliers detected by boxplot which are the values (256 GB and 512 GB) ,going back to the data these high storage values are not rare we cant consider them real outliers that need to be removed,but a minimum storage capacity observed is 1.0 GB considered to be an outlier and it will be preprocessed.

```{r}
boxplot(dataset_phones$battery , data = dataset_phones, xlab = "battery", ylab = "Frequancy")
```
```{r}
boxplot.stats(dataset_phones$battery)$out
```
The battery boxplot and statistical result illustrates that majority of the phone batteries in the dataset are high in range there’s a lot of outliers detected, they are estimated based on  boxplot and will be preprocessed.

```{r}
boxplot(dataset_phones$price.USD. , data = dataset_phones, xlab = "price(USD)", ylab = "Frequancy")
```


```{r}
boxplot.stats(dataset_phones$price.USD.)$out

```

The price(USD) boxplot and statistical result illustrates there’s outliers detected way higher than the maximum phone price in the dataset that will be removed in preprocessing. 

```{r}
boxplot(dataset_phones$ram.GB. , data = dataset_phones, xlab = "ram(GB)", ylab = "Frequancy")
```
```{r}
boxplot.stats(dataset_phones$ram.GB.)$out

```

The ram(GB) boxplot and statistical result illustrate that these outliers need to be smoothed for accurate results.
```{r}
boxplot(dataset_phones$weight.g. , data = dataset_phones, xlab = "Weight(G)", ylab = "Frequancy")
```
```{r}
boxplot.stats(dataset_phones$weight.g.)$out
```
The weight(G) boxplot and statistical result illustrate that the weight of phones is  close in values ranged between (130.0 g and 500.0 g) and there is outliers that need to be preprocessed for accurate results 
```{r}
boxplot(dataset_phones$inches , data = dataset_phones, xlab = "inches)", ylab = "Frequancy")
```
```{r}
boxplot.stats(dataset_phones$inches)$out

```
The inches box plot and statistical result illustrate that the inches of phones ranged between(3.800 and 10.400) and processing is required for outliers.



```{r}
hist(dataset_phones$weight.g.)


```
The weight(g) histogram graph indicates an asymmetric right-skewed distribution of phone weights, with the majority of phone weights falling between (150g and 200g) and outliers that require preprocessing for dependable results.

```{r}
hist(dataset_phones$price.USD.)

```

The price(USD) histogram graph demonstrates a right-skewed distribution most phones less than $500.

```{r}
hist(dataset_phones$battery)

```

The histogram illustrates that the distribution of phone batteries is generally symmetrical and that it falls within a normal range, which is between 2000 and 7000, with outliers that need to be smoothed out.
```{r}
hist(dataset_phones$storage.GB.)
```
The histogram illustrates that theres a variation in storage values ,and it also shows it has outliers that needs to be smoothed out.

```{r}
library(readr)
library(ggplot2)
ggplot(dataset_phones, aes(x =dataset_phones$battery_type , y = dataset_phones$price.USD.)) + 
  geom_bar(stat = "identity")
```
the battery type and price(USD) bar plot illustrates that the majority of phone batteries of the type ( Li – Po ) and it’s the battery type that was most paid for. Although that (Li – ion) has more advantages and is considered better, it wasn’t used in many phones as the other battery type.

```{r}
with(dataset_phones, plot(dataset_phones$price.USD., dataset_phones$battery))
```

```{r}
correlation <- cor(dataset_phones$battery, dataset_phones$price.USD.)
print(correlation)
```
The scatter plot of the price(USD) and battery shows no meaningful linear relationship between the price and battery capacity ,changes in the price of a product are not associated with changes in the battery capacity in any consistent way.

The correlation coefficient is very close to zero. This indicates that the attributes are independent ,But this could change when outliers are removed.

```{r}
with(dataset_phones, plot(dataset_phones$price.USD., dataset_phones$storage.GB.))

```

```{r}
correlation <- cor(dataset_phones$storage.GB., dataset_phones$price.USD.)
print(correlation)
```
The scatter plot and the measurements of correlation between price(USD) and storage(GB) suggests a relatively positive correlation. It's not close to 1, so the relationship between the variables is not very stronge.

```{r}
with(dataset_phones, plot(dataset_phones$ram.GB., dataset_phones$storage.GB.))
```


```{r}
correlation <- cor(dataset_phones$storage.GB., dataset_phones$ram.GB.)
print(correlation)

```
The scatter plot and correlation coefficient shows a strong positive relationship between the amount of storage(GB) and the amount of RAM(GB) .They rise togther, as storage capacity increases, the RAM capacity also tends to increase, and vice versa.

having a strong positive correlation between two variables does not necessarily mean you should drop one of them, dropping one completely may not be ideal as they serve different purposes but including both storage and RAM as features can potentially improve the classification performance of our model. Even if these features are correlated, they might still contain unique information that is relevant for distinguishing between different classes. 
```{r}
library(ggplot2)
ggplot(dataset_phones, aes(x = brand)) +
  geom_bar() +
  labs(title = "Counts of Brands", x = "Brand", y = "Count")
```
xiaomi is the most brand produce mobiles and Google is the least.

```{r}
ggplot(dataset_phones, aes(x = os)) +
  geom_bar() +
  labs(title = "Counts of Operating Systems", x = "Operating System", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
Android 11 and Android 10 is the most operating systems used in phones based on our data

Based on the analysis of the operating systems graph, it is clear that the "OS"(operating system) column in our dataset contains various forms of noise. Some phones have unspecified operating system values such as "Android," while others have improperly formatted entries like "Android 10 / Android 11" or "Android 12 or 13." ,after rechecking the data the number of tuples that has this type of noise is small ,we fixed the phones that use the operating system  "Android 10 / Android 11" or "Android 12 or 13." by replacing it with the latest operating system because we asume that the phone got updated ,and the phones that use the opearting system "Android" we replaced it by "Android 11" since its the most operating system used in our data.
```{r}
filtered_data <- dataset_phones[dataset_phones$os == 'Android 10/ Android 11', ]
print(filtered_data)

```

```{r}
dataset_phones[888,3]<-"Android 11"
```


```{r}
filtered_data <- dataset_phones[dataset_phones$os == 'Android', ]
print(filtered_data)
```

```{r}
dataset_phones[1163,3]<-"Android 11"
dataset_phones[1233,3]<-"Android 11"
dataset_phones[1473,3]<-"Android 11"
```

```{r}
filtered_data <- dataset_phones[dataset_phones$os == 'Android 12 or 13', ]
print(filtered_data)
```

```{r}
dataset_phones[1410,3]<-"Android 13"
dataset_phones[1412,3]<-"Android 13"
```



```{r}
barplot(table(dataset_phones$video_1080p))
```
The bar plot demonstrates that nearly all of the phones in our sample have a video resolution of  1080p, which is equivalent to a Full HD resolution. In essence, the resolution of a video or image tells you about the video's quality and how clear it is. As there are more pixels, a video with a higher resolution will contain images that are more clear and brighter. offering you higher video quality in the end.


```{r}
barplot(table(dataset_phones$video_480fps))
```
This bar graph illustrates that most phones lack 480fps video capabilities. The rate at which several frames appear within a second is known as the frame rate. 480 FPS indicates that the camera is taking 480 frames or photos in a single second. When you shoot at 480 frames per second, the video will play 16 times slower. This frame rate is regarded as the foundation of super slow motion. This is common when demonstrating exceedingly quick movements. The phones in our sample are unable to record videos in slow motion at 480 frames per second, making it impossible to capture and display incredibly quick movements.
```{r}
phone_name_counts <- table(dataset_phones$phone_name)
phone_name_counts_df <- as.data.frame(phone_name_counts)
colnames(phone_name_counts_df) <- c("Phone Name", "Count")
 print(phone_name_counts_df)
```

```{r}
filtered_data <- dataset_phones[dataset_phones$phone_name == 'V30', ]
print(filtered_data)
```
 
```{r}
filtered_data <- dataset_phones[dataset_phones$phone_name == '9', ]
print(filtered_data)
```
we noticed that there are many brands that has the same phone name. 
## -preprocessing


### dropping phone_name column
```{r}
dataset_phones <- dataset_phones[, !names(dataset_phones) %in% c("phone_name")]

```
 "phone name" in our data is a  unique identifier of the phone names as shown before and it don't carry meaningful information related to the predictive task that we are doing.
 
### checking duplication after removing phone names
```{r}
num_duplicates <- sum(duplicated(dataset_phones))
cat("Number of duplicate rows:", num_duplicates, "\n")
```

The removal of a unique identifier column might cause duplication but after rechecking our data the rows are not duplicated

### we can extract announcement_year column from announcement_date column
```{r}
dataset_phones$announcement_date <- as.character(dataset_phones$announcement_date)
 dataset_phones$announcement_year <- as.integer(sapply(strsplit(dataset_phones$announcement_date, '-'), function(x) x[1]))

```
We extracted the phones announcement year from the  announcement_date column to make data simpler,high in consistency and more focused to the predictive task we are making, since the day and month of the phone's announcement is not relevant the prices of the phone 

### drop announcement_date
```{r}
dataset_phones <- dataset_phones[, !names(dataset_phones) %in% c("announcement_date")]
```

### extract hight and width from resoultion

```{r}
dataset_phones$width <- as.integer(sapply(strsplit(dataset_phones$resolution, "x"), "[[", 1))
```

```{r}
dataset_phones$height <- as.integer(sapply(strsplit(dataset_phones$resolution, "x"), "[[", 2))

```
### droping resolution 
```{r}
dataset_phones <- dataset_phones[, !names(dataset_phones) %in% c("resolution")]
```
we split the resolution column into height and width as it would help give more detailed information about each characteristic as well as it would enable numerical operations to be done and it is easier to filter, compare and understand the correlation between each of them and the price



### checking for outliers in the new added attributes(width and height)
```{r}
boxplot(dataset_phones$width , data = dataset_phones, xlab = "width", ylab = "Frequancy")
```

```{r}
boxplot(dataset_phones$height, data = dataset_phones, xlab =" height"
        , ylab = "Frequancy")
```

```{r}
boxplot.stats(dataset_phones$height)$out
```

```{r}
boxplot.stats(dataset_phones$width)$out
```
only 2 phone with the width 3840 were dedicted and considerd outliers.

## removing outliers



```{r}

library(dplyr)
dataset_phones<- dataset_phones%>%
    filter(price.USD.<=1750,
           storage.GB. >= 8 & storage.GB. <= 512,
           ram.GB. >= 5 & ram.GB.< 24,
           battery >= 2500 & battery < 7000,
           width>=480 & width<3840)

```



### Encoding 
```{r}
dataset_phones$video_720p=as.integer(as.logical(dataset_phones$video_720p))
dataset_phones$video_1080p=as.integer(as.logical(dataset_phones$video_1080p))
dataset_phones$video_4K=as.integer(as.logical(dataset_phones$video_4K))
dataset_phones$video_8K=as.integer(as.logical(dataset_phones$video_8K))
dataset_phones$video_30fps=as.integer(as.logical(dataset_phones$video_30fps))
dataset_phones$video_60fps=as.integer(as.logical(dataset_phones$video_60fps))
dataset_phones$video_120fps=as.integer(as.logical(dataset_phones$video_120fps))
dataset_phones$video_240fps=as.integer(as.logical(dataset_phones$video_240fps))
dataset_phones$video_480fps=as.integer(as.logical(dataset_phones$video_480fps))
dataset_phones$video_960fps=as.integer(as.logical(dataset_phones$video_960fps))
dataset_phones$battery_type=factor(dataset_phones$battery_type,levels = c("Li-Ion","Li-Po"),labels = c(0,1))
```
we noticed that there is logical attributes,and a character attribute with only two value only so we decided to encode the values so it will help simplify the data representation , also now after the encoding we can do calculations on the values.
### Normalization
```{r}
normalize <- function(x) { return((x - min(x))/ (max(x) - min(x)))}
dataset_phones$weight.g. <- normalize(dataset_phones$weight.g.) 
dataset_phones$inches <- normalize(dataset_phones$inches)
dataset_phones$battery <- normalize(dataset_phones$battery)
dataset_phones$width <- normalize(dataset_phones$width)
dataset_phones$height <- normalize(dataset_phones$height)
dataset_phones$storage.GB. <- normalize(dataset_phones$storage.GB.)
dataset_phones$ram.GB. <- normalize(dataset_phones$ram.GB.)
```
When features have significantly different scales, leading to biased results Normalization ensures that no single feature disproportionately influences the analysis. 
The attributes inches, weight, and battery were normalized and scaled to accept values between 0 and 1, where each of these qualities will have an equal weight. These two stages greatly simplify and help in handling the data.

### Descritisation and labeling price
```{r}
dataset_phones$price=cut (dataset_phones$price.USD., breaks=c(40,610, 1180, 2300), labels=c("cheap","affordable","expensive") )
```
our dataset was considered as regression so we decided to discretize the price column and  add labels to the prices and divide them into 3 categories (“Cheap”, “ Affordable”, and “Expensive”) which would be more fitting, as well as it transformed our dataset from being regression into classification





## Data after preprocessing
```{r}
summary(dataset_phones)

```

## Feature selection
### measuring chi-square
```{r}
result=chisq.test(dataset_phones$price.USD. , dataset_phones$phone_name)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$os)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$brand)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$battery_type)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_720p)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_1080p)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_4K)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_8K)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_30fps)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_60fps)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_120fps)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_240fps)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_480fps)
print(result)
result=chisq.test(dataset_phones$price.USD. , dataset_phones$video_960fps)
print(result)


result2=cor(dataset_phones$price.USD. ,dataset_phones$inches)
print(result2)
result2=cor(dataset_phones$price.USD. ,dataset_phones$battery)
print(result2)
result2=cor(dataset_phones$price.USD. ,dataset_phones$ram.GB.)
print(result2)
result2=cor(dataset_phones$price.USD. ,dataset_phones$weight.g.)
print(result2)
result2=cor(dataset_phones$price.USD. ,dataset_phones$storage.GB.)
print(result2)
result2=cor(dataset_phones$price.USD. ,dataset_phones$width)
print(result2)
result2=cor(dataset_phones$price.USD. ,dataset_phones$height)
print(result2)
```
After looking at the results of chi-square we can see that from the nominal attributes (os) and from numerical attributes (width)  represents the most important attribute that contributes to the price on the other hand the  nominal attributes (video_1080p) and from numerical attributes (battery) is the least relevant to the price even though  
it has a lower  value but it still contributes to the price so we dont consider the need for feature selection .
```{r}
result=cor(dataset_phones$price.USD. ,dataset_phones$ram.GB.)
print(result)
result=cor(dataset_phones$price.USD. ,dataset_phones$weight.g.)
print(result)
result=cor(dataset_phones$price.USD. ,dataset_phones$width)
print(result)
result=cor(dataset_phones$price.USD. ,dataset_phones$height)
print(result)
result=cor(dataset_phones$price.USD. ,dataset_phones$inches)
print(result)

```
the correlation of the attributes (ram,weight,width,height,inches)with the price shows that there is a positive relationship  which mean that they will help in the classification process of the class lable which is price. 
### dropping price.USD after labeling the data to build the classifier

```{r}
dataset_phones <- dataset_phones[, !names(dataset_phones) %in% c("price.USD.")]
```

### immbalance dataset
```{r}

ggplot(dataset_phones, aes(x = price)) +
  geom_bar() +
  labs(title = "Counts of price", x = "price", y = "Count")
```
based on the class lable plot it shows that our dataset is imbalanced and that will effect the classification task this will result in a significant impact on the performance of a classification model, the model may become biased towards the majority class (in this case, "cheap" items). It may focus more on learning how to classify the majority class accurately, often ignoring the minority classes.the model may perform well on the majority class but poorly on the minority classes and the accuracy will be misleading.



## Classification
### converting to factor
```{r}
dataset_phones$announcement_year <- as.factor(dataset_phones$announcement_year)
dataset_phones$battery_type <- as.factor(dataset_phones$battery_type)
dataset_phones$video_720p <- as.factor(dataset_phones$video_720p)
dataset_phones$video_1080p <- as.factor(dataset_phones$video_1080p)
dataset_phones$video_4K <- as.factor(dataset_phones$video_4K)
dataset_phones$video_8K <- as.factor(dataset_phones$video_8K)
dataset_phones$video_30fps <- as.factor(dataset_phones$video_30fps)
dataset_phones$video_60fps <- as.factor(dataset_phones$video_60fps)
dataset_phones$video_120fps <- as.factor(dataset_phones$video_120fps)
dataset_phones$video_240fps <- as.factor(dataset_phones$video_240fps)
dataset_phones$video_480fps <- as.factor(dataset_phones$video_480fps)
dataset_phones$video_960fps <- as.factor(dataset_phones$video_960fps)
dataset_phones$brand <- as.factor(dataset_phones$brand)
dataset_phones$os <- as.factor(dataset_phones$os)
```

### load the data after preprocessing to a CVS file
```{r}
write.csv(dataset_phones, file = "dataset_phones_classification.csv" ,row.names = FALSE)
```
### load the processed data
```{r}
dataset_phones_2 <- read.csv("dataset_phones_classification.csv" ,header = TRUE,sep=',', stringsAsFactors = T)
```

### laod packages 
```{r}
install.packages("party")
install.packages('caret')
install.packages('ROSE')
install.packages('rpart.plot')
```
```{r}
library(party)
library(caret)
```

## INFORMATION GAIN(ID3) (70% 30%)

### Set seed for reproducibility and Sample the dataset into training and test sets using the holdout method (70% training  - 30% testing)
### the function(ctree) from package (party) used to measure information gain(ID) as a splitting criteria
```{r}
set.seed(1234)
ind <- sample(2, nrow(dataset_phones_2), replace=TRUE, prob=c(0.7, 0.3))
trainData <- dataset_phones_2[ind==1,]
testData <- dataset_phones_2[ind==2,]
```

### Define the formula for the classification tree
```{r}
myFormula <- price ~ brand+os +inches+battery+battery_type+ram.GB.+weight.g.+storage.GB.+ video_720p +video_1080p+video_4K+video_8K+ video_30fps+video_60fps+video_120fps+ video_240fps+ video_480fps+video_960fps+ announcement_year+width+height
```

```{r}
dataset_phones_c <- ctree(myFormula, data = trainData )

```
### Decision tree
```{r}
plot(dataset_phones_c)
plot(dataset_phones_c, type="simple")
```
### Confusion matrix
```{r}
testPred <- predict(dataset_phones_c, newdata = testData)
table(testPred, testData$price)
```

```{r}
results <- confusionMatrix(testPred, testData$price)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)

```

We first started constructing the tree by using a 70 - 30 partition strategy,where 70% of the data was used for training the model, and the remaining 30% was reserved for evaluating its performance. The decision to use a 70-30 split is a commonly employed approach for finding a balance between having enough training data for the model to learn and having enough data to evaluate its generalization abilities.The root of the decision tree is the "width" feature, which has the highest information gain. This means that the model, during training, found "width" to be the most significant feature for splitting and making predictions

However, the decision tree constructed from this process was not accurate in classifying phones as "expensive." It focused primarily on distinguishing between "affordable" and "cheap" categories and did not specifically consider the "expensive" class. As a result, when using this tree to classify a phone, it wouldn't classify it as "expensive." This limitation was due to the imbalance in the dataset, which contained only 25 instances labeled as "expensive."

The overall accuracy is 87.31%, but its misleading due to the class imbalance or it could be due to the method we chose to partition our dataset the training data didn't include any expensive phones for the classifier to learn from. 

The model's performance was evaluated using sensitivity and specificity values for each class. The "affordable" class had a sensitivity of 22.85%, indicating only 22.85% of positive instances were correctly identified(True Positives = 8).The model identified 99.12% of negative instances in this category.
The "cheap" class had a sensitivity of 99.64%, indicating most of the positive instances were correctly identified,confusion matrix shows that the model has correctly predicted most of the instances of the "cheap" class (True Positives = 274)
The "expensive" class had a sensitivity of 0%, indicating none of positive instances were correctly identified. However, the model identified all negative instances as negative. These results highlight the model's effectiveness in identifying positive and negative instances.

In conclusion, the decision tree's split based on available attributes is not accurate in classifying phones as "expensive," due to the limited presence of expensive instances in the dataset and the lack of consideration for the expensive class during the splitting process.

### Creating a decision tree using the gain ratio(C5.0) as the splitting criterion 
### the algorithm (C5.0) from package C50 used to measure the gain ratio(using 70% and 30% partitioning) and its an improvements to C4.5 algorithm
```{r}
install.packages("C50")
library(C50)
library(printr)
```

```{r}
# Build a C5.0 decision tree
dataset_phones_c <- C5.0(price ~ ., data = trainData)
```

```{r}
# Plot the decision tree (optional)
plot(dataset_phones_c)
```

```{r}
# Make predictions on the test set
testPred <- predict(dataset_phones_c, newdata = testData)
```

```{r}
# Create a confusion matrix
table(testPred, testData$price)
```
### print tree rules
```{r}
summary(dataset_phones_c)
```

```{r}
# Print accuracy
results <- confusionMatrix(testPred, testData$price)
acc <- results$overall["Accuracy"] * 100
results

```
```{r}
# Precision calculations
precision_affordable <- 7 / (7 + 28)
precision_cheap <- 275 / (275 + 0)
precision_expensive <- 0 / (0 + 0)



cat("Precision for affordable:", precision_affordable, "\n")
cat("Precision for cheap:", precision_cheap, "\n")
cat("Precision for expensive:", precision_expensive, "\n")
```

 
 The decision tree has "width" as the root attribute, chosen based on the highest gain ratio. This decision tree is simpler than the previous one, requiring only four splitting attributes used to classify a phone price  the matrix shows improved sensitivity for the "affordable" class, with 7 instances correctly identified. However, the model still struggles to classify instances of the "expensive" class, with sensitivity remaining at 0 precision for cheap class is 1 When the model predicts an instance as "cheap," it is highly accurate, with no false positives observed in this case and this due the the majority of cheap phones in the training dataset . overall accuracy has increased to approximately 87.31%, representing an improvement over the previous model but its misleading because it couldn't classify any afforadble and expensive phones due to the imbalanced data we are working with.The model lacks information about these classes during training, leading to a failure in their identification during testing.
 
 
### GINI INDEX (CART)
```{r}
library(rpart)
library(rpart.plot)
```

```{r}
fit.tree = rpart(price ~ ., data=trainData, method = "class", cp=0.008)
fit.tree
```
### plotting the tree
```{r}
rpart.plot(fit.tree)
```
### Confusion matrix
```{r}
pred.tree = predict(fit.tree, testData, type = "class")
table(pred.tree,testData$price)
```

```{r}
library(caret)
results <- confusionMatrix(pred.tree, testData$price)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

Using the same partitioning strategy of 70-30, a decision tree was constructed based on the Gini index as the selection measure. The Gini index calculates the impurity or uncertainty at each node, and the attribute with the lowest Gini index is chosen as the splitting criterion. This measure helps in identifying the attributes that provide the most significant information gain and result in higher purity in the resulting subsets.

The decision tree utilized the "Width" attribute as the initial splitting criterion, selected due to its lowest Gini index value among the candidate attributes. Additional splitting criteria, such as "Brand," "Video 240fps," and "Weight," were chosen based on their respective Gini index values. This approach aimed to create splits that maximize the purity of the resulting subsets and improve the model's ability to classify instances accurately.

Similar to the decision tree constructed using information gain, the Gini index-based decision tree did not classify any instances into the "expensive" class. This observation reinforces the imbalance present in the dataset, where the majority of instances are categorized as either "affordable" or "cheap." It highlights the challenge faced by the model in accurately classifying instances in the underrepresented "expensive" class.

The Gini index-based decision tree achieved an accuracy of 85.13%, slightly lower than the accuracy of the ID3 tree. Accuracy measures the overall correctness of the model's classifications, indicating that 85.13% of the instances were correctly classified. Although the accuracy decreased slightly, it remains a reasonably high level of accuracy.

To evaluate the model's performance, sensitivity and specificity values were calculated for each class. For the "affordable" class, the model exhibited higher specificity (95.48%) than sensitivity (25.71%). This implies that the model is better at correctly identifying instances that belong to the negative class for this category, but it struggles to accurately identify instances that belong to the positive class. The model tends to be more conservative in labeling instances as positives for the "affordable" class. In the case of the "cheap" class, the model demonstrated excellent sensitivity (96.73%) in correctly identifying instances that belong to the positive class. However, its specificity (27.08%) was relatively low, indicating that the model is prone to false positives and struggles to accurately identify instances that belong to the negative class for this category. For the "expensive" class, the model exhibited perfect specificity (100%) in correctly identifying instances that belong to the negative class. However, its sensitivity (0%) was zero, indicating that the model failed to identify any instances that belong to the positive class for this category.

In summary, the decision tree based on the Gini index faced similar challenges to the decision tree based on information gain when it came to accurately classifying instances in the "affordable" and "cheap" classes. The model's performance varied in terms of sensitivity and specificity for each class, highlighting its strengths and weaknesses in differentiating between positive and negative instances.


##information gain (80% - 20%)
## -------------------------------------------------------------------------------------------------------------------------------
### Set seed for reproducibility and Sample the dataset into training and test sets using the holdout method (80% training  - 20% testing)
### the function(ctree) from package (party) used to measure information gain(ID) as a splitting criteria
```{r}
set.seed(1234)
ind <- sample(2, nrow(dataset_phones_2), replace=TRUE, prob=c(0.8, 0.2))
trainData <- dataset_phones_2[ind==1,]
testData <- dataset_phones_2[ind==2,]
```

```{r}
myFormula <- price ~ brand + os + inches + battery+ battery_type+ ram.GB.+ weight.g.+storage.GB.+ video_720p + video_60fps+ video_1080p+ video_30fps+ video_480fps+video_240fps+video_120fps+ video_960fps+ video_4K+video_8K +announcement_year+width+height
```

```{r}
install.packages("party")
library(party)
install.packages('caret')
library(caret)
phones_ctree <- ctree(myFormula, data=trainData)
table(predict(phones_ctree), trainData$price)
print(phones_ctree)
plot(phones_ctree)
```

```{r}
testPred <- predict(phones_ctree, newdata = testData)
table(testPred, testData$price)
```

```{r}
library(caret)
results <- confusionMatrix(testPred, testData$price)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)

```

The second partition strategy, we used 80 - 20 split where 80% of the data was used for training the model and 20% for testing its performance. This split provides sufficient training data for effective model learning, striking a balance to avoid overfitting and ensuring computational efficiency. 

In the constructed decision tree, the splitting attribute at the root level was the "Width" attribute, chosen because it has the highest information gain. The subsequent splitting criteria at the second level were determined by the conditions of the "Operating System" attribute and the "Video 30fps" attribute, depending on whether the width of the phone was smaller or larger than 0.365. Finally, the last level of splitting criteria involved both the "Operating System" and "Video 240fps" attributes.

The data used to train the model showed an unbalanced distribution, as can be seen by examining the decision tree and confusion matrix. The majority of instances were classified into the "cheap" class, with only instances meeting specific conditions (a width larger than 0.365 and a "Video 30fps" value of false) being classified as "affordable." Notably, the tree did not classify any instances as "expensive" due to the overwhelming prevalence of the "cheap" class in the dataset.

The accuracy of the decision tree was calculated to be 87.44%, slightly higher than the 70-30 ID3 tree. While this accuracy rate demonstrates that the model is performing reasonably well in terms of overall accuracy. However, it also indicates that there are some class values that the model struggles to correctly classify, particularly "affordable" and "expensive."

The model's performance is assessed through sensitivity and specificity values for distinct classes. In the "affordable" class, the model demonstrates a sensitivity of 17.39%, accurately identifying 17.39% of actual positive instances, while achieving a high specificity of 99.5%, signifying precision in recognizing actual negative instances. Conversely, for the "cheap" class, the model excels with a 100% sensitivity, correctly identifying all actual positives, but struggles with a specificity of 15.62%, indicating challenges in pinpointing actual negatives. In the case of the "expensive" class, the model shows a 0% sensitivity, implying difficulty in identifying positive instances, paired with a 100% specificity, showcasing accuracy in recognizing negatives. These sensitivity and specificity metrics offer a detailed perspective on the model's strengths and weaknesses across different classes.

The decision tree demonstrates reasonably good overall accuracy but faces challenges in correctly classifying instances for certain classes. The values for specificity and sensitivity provide insight into how well the model performs for each class.

### Creating a decision tree using the gain ratio(C5.0) as the splitting criterion 
### the function (C5.0) from package C50 used to measure the gain ratio(using 80% and 20% partitioning)
```{r}

install.packages("C50")
library(C50)
library(printr)
install.packages('caret')
library(caret)
install.packages('ROSE')
install.packages('rpart.plot')
library(rpart)
```



```{r}
# Build a C5.0 decision tree
dataset_phones_c <- C5.0(price ~ ., data = trainData)
```

```{r}
# Plot the decision tree (optional)
plot(dataset_phones_c)
```

```{r}
# Make predictions on the test set
testPred <- predict(dataset_phones_c, newdata = testData)
```



```{r}
summary(dataset_phones_c)
```

```{r}
# Create a confusion matrix
table(testPred, testData$price)
```

```{r}
# Print accuracy
results <- confusionMatrix(testPred, testData$price)
acc <- results$overall["Accuracy"] * 100
results

```

```{r}
# Precision calculations
precision_affordable <- 2 / (2 + 21)
precision_cheap <- 191 / (191 + 0)
precision_expensive <- 0 / (0 + 8)

cat("Precision for affordable:", precision_affordable, "\n")
cat("Precision for cheap:", precision_cheap, "\n")
cat("Precision for expensive:", precision_expensive, "\n")
```



The decision tree is constructed with the "width" attribute as the root, selected based on the highest gain ratio measured by the algorithm. The model exclusively utilizes the width of the phone to categorize prices. While it achieves the highest overall accuracy compared to other classifiers in this task (87.6%), it demonstrates limitations, particularly in accurately identifying "affordable" and "expensive" phones due to the imbalanced dataset which result in having a misleading classification of phones.

The reliance on a single feature, "width," raises practical concerns in real-world scenarios. In phone price classification, numerous other features, such as storage capacity and battery type, are typically considered. Despite the high accuracy, the model's singular focus on "width" makes it potentially misleading and unsuitable for practical use in classifying new data.

Sensitivity is particularly low for "affordable" (8.70%) and "expensive" (0.00%), indicating the model struggles to correctly identify instances of these classes.Specificity is high for "affordable" (99.50%) and "expensive" (100.00%), suggesting that the model performs well in correctly classifying instances as not belonging to these classes,However, specificity is low for "cheap" (9.38%), indicating a high rate of false positives for this class.
based on the precision measures which is a metric that assesses the accuracy of the positive predictions made by a classification model.Out of all instances predicted as "affordable," only a small proportion were truly "affordable." The model may have a tendency to include false positives when predicting "affordable" instances  When the model predicts an instance as "cheap," it is highly accurate, with no false positives in this case.The model did not correctly predict any instances as "expensive," and there are false positives. However, precision cannot be computed due to the absence of true positives. 

In practice, a more comprehensive approach considering multiple relevant features would be essential for developing a robust and reliable classifier.



### GINI INDEX (CART)
```{r}
library(rpart)
library(rpart.plot)
```

```{r}
fit.tree = rpart(price ~ ., data=trainData, method = "class", cp=0.008)
fit.tree
```
### plotting the tree
```{r}
rpart.plot(fit.tree)
```
### Confusion matrix
```{r}
pred.tree = predict(fit.tree, testData, type = "class")
table(pred.tree,testData$price)
```

```{r}
library(caret)
results <- confusionMatrix(pred.tree, testData$price)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

Using the same partitioning strategy of 80-20, a decision tree was constructed using the Gini index as the selection measure. The decision tree used the "Width" attribute as the initial splitting criterion, chosen because it had the lowest Gini index value among the candidate attributes. Additional splitting criteria, such as "Brand," "Video 960fps," and "Weight," were selected based on their respective Gini index values.

Similar to the decision tree constructed using information gain, the decision tree based on the Gini index also did not classify any data into the "expensive" class. This observation further highlights the imbalanced nature of the dataset, with the majority of instances being classified as either "affordable" or "cheap." 

The Gini index-based decision tree achieved an accuracy of 86.54%, which is slightly lower than the accuracy of the ID3 tree. Accuracy is a measure of the model's overall correctness in classifying instances, and in this case, it indicates that 86.54% of the instances were correctly classified. While the accuracy is slightly lower compared to the ID3 tree, it is still a reasonably high accuracy level.

To evaluate the model's performance, sensitivity and specificity values were calculated for each class. For the "affordable" class, the model exhibited higher specificity (97%) than sensitivity (17.39%). This means that the model is better at correctly identifying instances that belong to the negative class for this class, but it struggles to accurately identify instances that belong to the positive class. The model tends to be more conservative in labeling instances as positives for the "affordable" class. In the case of the "cheap" class, the model demonstrated excellent sensitivity (98.95%) in correctly identifying instances that belong to the positive class. However, its specificity (25%) was relatively low, indicating that the model is prone to false positives and struggles to accurately identify instances that belong to the negative class for this class.For the "expensive" class, the model exhibited perfect specificity (100%) in correctly identifying instances that belong to the negative class. However, its sensitivity (0%) was zero, indicating that the model failed to identify any instances that belong to the positive class for this class.

In summary, the decision tree based on the Gini index selection measure also faced challenges in accurately classifying instances for the "affordable" and "cheap" classes, similar to the decision tree based on information gain. The model demonstrated varying performance in terms of sensitivity and specificity for each class, indicating areas where it excelled and areas where it struggled.

##information gain (90% - 10%)
## -------------------------------------------------------------------------------------------------------------------------------
### Set seed for reproducibility and Sample the dataset into training and test sets using the holdout method (90% training  - 10% testing)
### the function(ctree) from package (party) used to measure information gain(ID) as a splitting criteria
```{r}
set.seed(1234)
ind <- sample(2, nrow(dataset_phones_2), replace=TRUE, prob=c(0.9, 0.1))
trainData <- dataset_phones_2[ind==1,]
testData <- dataset_phones_2[ind==2,]
```

```{r}
myFormula <- price ~ brand + os + inches + battery+ battery_type+ ram.GB.+ weight.g.+storage.GB.+ video_720p + video_60fps+ video_1080p+ video_30fps+ video_480fps+video_240fps+video_120fps+ video_960fps+ video_4K+video_8K +announcement_year+width+height
```

```{r}
install.packages("party")
library(party)
install.packages('caret')
library(caret)
phones_ctree <- ctree(myFormula, data=trainData)
table(predict(phones_ctree), trainData$price)
print(phones_ctree)
plot(phones_ctree)
```

```{r}
testPred <- predict(phones_ctree, newdata = testData)
table(testPred, testData$price)
```


```{r}
library(caret)
results <- confusionMatrix(testPred, testData$price)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)

```

For the last partition strategy, we used 90 - 10 split where 90% of the data was used for training the model and 10% for testing its performance. since the dataset is relatively small, using a larger training set can help to provide more data for the model to learn from. With a 90% training set, the model has access to a greater number of instances to capture patterns and relationships in the data.

The decision tree was constructed with the "Width" attribute as the splitting attribute at the root level, chosen for its high information gain. At the second level, the splitting criteria were determined by the conditions of the "Operating System" and "Video 30fps" attributes, based on the width of the phone. The third level involved the "Video 240fps" and "Brand" attributes, while the last level incorporated both the "Brand" and "Video 30fps" attributes.

However, despite the large training set, the decision tree did not classify any instances into the "expensive" class. This suggests that the "expensive" class is underrepresented in the dataset, with a majority of instances labeled as "cheap." Imbalanced datasets can pose challenges for decision trees, as they may prioritize classifying instances into the more prevalent classes, leading to difficulties in accurately predicting the minority class. The small size of the dataset might have also limited the model's ability to effectively capture the complexity of the "expensive" class. With fewer representative instances during training, the decision tree may not have learned the necessary patterns and rules required for accurate classification of the "expensive" class.

The accuracy of 87.59% indicates that the decision tree correctly classified approximately 87.59% of the instances in the dataset, which is relatively good overall. It outperformed other ID3 trees in terms of accuracy, making it the most accurate choice for this dataset.

Analyzing sensitivity and specificity values for different classes provides further insights into the model's performance. In the "affordable" class, the model had a sensitivity of 20%, correctly identifying 20% of actual positive instances, and a high specificity of 99.12%, indicating precision in recognizing actual negatives. For the "cheap" class, the model achieved a 100% sensitivity, accurately identifying all actual positives, but struggled with a specificity of 21.05%, suggesting challenges in identifying actual negatives. In the case of the "expensive" class, the model had a 0% sensitivity, implying difficulty in identifying positive instances, but achieved a specificity of 100%, indicating accuracy in recognizing negatives.

These sensitivity and specificity metrics offer a detailed understanding of the model's strengths and weaknesses across different classes, highlighting its ability to accurately classify instances within certain classes while exposing areas for improvement, particularly in the sensitivity of the "expensive" class.

In summary, the 90-10 split used for training and testing the model has provided valuable insights into its performance. While the decision tree faced challenges in classifying the underrepresented "expensive" class due to dataset imbalance, it still achieved a high overall accuracy. By analyzing the model's sensitivity and specificity for each class, we can gain a deeper understanding of its strengths and weaknesses.

### Creating a decision tree using the gain ratio(C5.0) as the splitting criterion 
### the function (C5.0) from package C50 used to measure the gain ratio(using 90% and 10% partitioning)

```{r}

install.packages("C50")
library(C50)
library(printr)
# Build a C5.0 decision tree
dataset_phones_c <- C5.0(price ~ ., data = trainData)
```

```{r}
# Plot the decision tree 
plot(dataset_phones_c)
```



```{r}
# Make predictions on the test set
testPred <- predict(dataset_phones_c, newdata = testData)
```

```{r}
# Create a confusion matrix
table(testPred, testData$price)
```

###  Print the decision rules
```{r}
summary(dataset_phones_c)
```
```{r}
# Precision calculations
precision_affordable <- 2 / (2 + 13)
precision_cheap <- 110 / (110 + 0)
precision_expensive <- 1 / (1 + 2)

cat("Precision for affordable:", precision_affordable, "\n")
cat("Precision for cheap:", precision_cheap, "\n")
cat("Precision for expensive:", precision_expensive, "\n")

```

```{r}
# Print accuracy
results <- confusionMatrix(testPred, testData$price)
acc <- results$overall["Accuracy"] * 100
results

```

The decision tree employs the "width" attribute as the root, chosen based on the highest gain ratio measured by the algorithm. The model relies exclusively on the phone's width and brand to classify the phone's prices.the accuracy matches the previous tree that used the information gain as the splitting criteria 87.6% .the model is misleading it struggles to classify expensive and afforadble phones,The sensitivity and specificity values further highlight the model's strengths and weaknesses for each class. For instance, while the model demonstrates perfect sensitivity for "cheap" phones, it struggles with "affordable" and "expensive" phones The trade-off between sensitivity and specificity suggests the need for a more balanced approach, incorporating additional features for a more robust and reliable classification model.


### Gini index
## classifiaction tree:
```{r}
library(rpart)
library(rpart.plot)
```

```{r}
fit.tree = rpart(price ~ ., data=trainData, method = "class", cp=0.008)
fit.tree
```

```{r}
rpart.plot(fit.tree)
```
```{r}
pred.tree = predict(fit.tree, testData, type = "class")
table(pred.tree,testData$price)
```
```{r}

library(caret)
results <- confusionMatrix(pred.tree, testData$price)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

Using the same 90-10 partitioning strategy, a decision tree was constructed using the Gini index as the selection measure. The Gini index-based decision tree aimed to address the limitations observed in previous decision trees and considered the "expensive" class during classification.

The decision tree utilized the "Width" attribute as the initial splitting criterion, selected for its lowest Gini index value among the candidate attributes. Additional splitting criteria, including "Brand," "Operating System," and "Weight," were chosen based on their respective Gini index values. This approach aimed to optimize the decision tree's ability to accurately classify instances across different classes, including the "expensive" category.

The accuracy of the Gini index-based decision tree is 86.82%, slightly lower than the accuracy of the ID3 tree. The accuracy metric measures the proportion of correctly classified instances out of the total number of instances in the dataset. While the accuracy may have decreased slightly, it is important to note that the Gini index-based tree successfully considered the "expensive" class, which was not the case with the previous decision trees.

To evaluate the model's performance more comprehensively, sensitivity and specificity values were analyzed for each class. For the "affordable" class, the model demonstrated a higher specificity (94.73%) than sensitivity (33.33%), indicating that it was better at identifying actual negatives than actual positives. This suggests that the model was more conservative in labeling instances as "affordable" and may have been cautious in classifying positives within this class. In contrast, for the "cheap" class, the model exhibited excellent sensitivity (95.45%), correctly identifying a high proportion of actual positives. However, its specificity (52.63%) was relatively lower, implying challenges in accurately identifying actual negatives. This indicates that the model was more prone to false positives when classifying instances as "cheap." In terms of the "expensive" class, the model achieved a high specificity (98.4%) for identifying actual negatives. However, its sensitivity (50%) was moderate, suggesting that it faced difficulties in capturing a significant proportion of actual positives. Although the model was better at recognizing negatives for the "expensive" class, it still had room for improvement in identifying positives accurately.

Overall, the Gini index-based decision tree addressed the limitation of previous decision trees by considering the "expensive" class during classification. While the accuracy slightly decreased, the model's performance was evaluated comprehensively through sensitivity and specificity metrics. This analysis revealed that the model exhibited varying strengths and weaknesses across different classes.


### load the data after preprocessing to a CVS file
```{r}
write.csv(dataset_phones, file = "dataset_phones_clustering.csv" ,row.names = FALSE)
```
### load the processed data
```{r}
dataset_phones_3 <- read.csv("dataset_phones_clustering.csv" ,header = TRUE,sep=',', stringsAsFactors = T)
```


##encoding
```{r}

dataset_phones_3$brand=factor(dataset_phones$brand,levels = c("Apple","Google","Honor","Huawei","Lenovo","LG","OnePlus","Oppo","Realme","Samsung","Sony","Vivo","Xiaomi"),labels = c(1,2,3,4,5,6,7,8,9,10,11,12,13))
dataset_phones_3$os <- factor(dataset_phones_3$os, levels = c("Android 10", "Android 11", "Android12", "Android12L", "Android 13", "Android 6", "Android 7.0", "Android 7.1", "Android 7.1.1", "Android 7.1.2", "Android 8.0", "Android 8.1",  "Android 9.0", "EMUI 12", "EMUI 13", "iOS 14.1", "iOS 15", "iOS 16"), labels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18))

```

We select column that only contains numeric values without the class lable, to ensure that the algorithm operates on a consistent and compatible data representation. 

```{r}
dataset_phones_3$brand <- as.numeric(dataset_phones_3$brand)
dataset_phones_3$os <- as.numeric(dataset_phones_3$os)
dataset_phones_3<- dataset_phones_3[, sapply(dataset_phones_3, is.numeric)]
```
# Check for missing values in dataset_phones_3 and remove it
```{r}
sum(is.na(dataset_phones_3))
dataset_phones_3 <- dataset_phones_3[complete.cases(dataset_phones_3), ]
```

##calculate k-mean 
```{r}
dataset_phones_3 <- scale(dataset_phones_3)
 km <- kmeans(dataset_phones_3, 2, iter.max = 140 , algorithm="Lloyd", nstart=100) 
 km
#total within-cluster-sum of square
twss <- sum(km$withinss)
print(twss)

```

We created the first size k cluster with k-mean algorithm using k=2, we also calculated the total within-cluster-sum of square(TWSS) which in this case equals 17952.6
The goal of the k-means clustering algorithm is to minimize the total within-cluster sum of squares(TWSS) in this case it didn't accomplish the desired goal as the TWSS is so high compared to other clusters with different value k.

```{r}
install.packages("factoextra")
library(factoextra)

```
##visualization of cluster
```{r}
fviz_cluster(list(data = dataset_phones_3, cluster = km$cluster),              ellipse.type = "norm", geom = "point", stand = FALSE,              palette = "jco", ggtheme = theme_classic())
library(cluster)
avg_sil <- silhouette(km$cluster,dist(dataset_phones_3))
fviz_silhouette(avg_sil)

```

From the plot, we are able to determine that certain data points do not belong to any specific cluster, which makes them outliers. These outliers indicate something unusual or abnormal in the data. However, one cluster is noticeably larger than the other. 

When we calculate and plot the average silhouette, we see that both cluster's have positive values but the second one has a little negative values in it which indicates that part of the second cluster doesn't fit well within any cluster and may have distinct characteristics or patterns. 

It is worth noting that the clusters overlap to some extent, but the overlap is not significant. As well as, the distance of cluster points within each cluster is generally small, indicating a high level of compactness within the clusters with one cluster being larger than the other.

The max size for the biggest cluster is 729, while the min size for the smallest cluster is 349, which indicates that the difference is big between values.

##BCubed precision and recall
```{r}
cluster_assignments <- c(km$cluster)
ground_truth_labels <- c(dataset_phones$price)

if (length(cluster_assignments) > length(ground_truth_labels)) {
  cluster_assignments <- cluster_assignments[1:length(ground_truth_labels)]
} else if (length(cluster_assignments) < length(ground_truth_labels)) {
  ground_truth_labels <- ground_truth_labels[1:length(cluster_assignments)]
}

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0

  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
# Count the total number of items in the same cluster
total_same_cluster <- sum(data$cluster == cluster)
    
# Count the total number of items with the same category
total_same_category <- sum(data$label == label)
    
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }

  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n

  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")

```
The BCubed Precision and Recall scores are metrics used to evaluate the performance of clustering algorithms.The BCubed Precision score for two clusters is 0.7827493 , indicating that, approximately 78.27% of the instances assigned to the same cluster are actually similar or related to each other. This means that the algorithm demonstrates a somewhat high level of precision in correctly identifying similar instances within clusters.

The BCubed Recall score is 0.5740784, which means that around 57.4% of the similar or related instances within each true cluster are correctly assigned to the same cluster. This indicates that the algorithm demonstrates a moderate level of recall in capturing instances that are truly related within clusters.


##calculate k-mean 
```{r}
dataset_phones_3 <- scale(dataset_phones_3)
 km <- kmeans(dataset_phones_3, 6, iter.max = 140 , algorithm="Lloyd", nstart=100) 
 km
#total within-cluster-sum of square
twss <- sum(km$withinss)
print(twss)
```
After applying the k-means algorithm with a cluster value of 6, we created the second size k cluster. When calculating the total within-cluster sum of squares (TWSS), we got a value of 12635.07 The objective of the k-means algorithm is to minimize the TWSS. In this case, the algorithm did not achieve the desired goal as the TWSS remains relatively high compared to other clusters with different values of k.

##visualization of cluster
```{r}
fviz_cluster(list(data = dataset_phones_3, cluster = km$cluster),              ellipse.type = "norm", geom = "point", stand = FALSE,              palette = "jco", ggtheme = theme_classic())
library(cluster)
avg_sil <- silhouette(km$cluster,dist(dataset_phones_3))
fviz_silhouette(avg_sil)
```
From our analysis, we have identified certain data points that cannot be assigned to any specific cluster. These data points are considered outliers, indicating potential irregularities or abnormalities. Additionally, the clusters exhibit different shapes, suggesting that they vary in size.

After calculating and visualizing the average silhouette, we can see that most of the clusters consist primarily of positive values. This implies that these clusters do align well with a particular group ,as only two clusters contain negative values.

We can also see that the clusters in the middle overlap to some extent. As well as, the distance of the points within each cluster are generally small, indicating a high level of compactness within the clusters as they are almost the same size excluding one cluster being somewhat bigger than the rest.

The largest cluster has a maximum size of 388, while the smallest cluster has a minimum size of 35	.


#BCubed precision and recall
```{r}
cluster_assignments <- c(km$cluster)
ground_truth_labels <- c(dataset_phones$price)

if (length(cluster_assignments) > length(ground_truth_labels)) {
  cluster_assignments <- cluster_assignments[1:length(ground_truth_labels)]
} else if (length(cluster_assignments) < length(ground_truth_labels)) {
  ground_truth_labels <- ground_truth_labels[1:length(cluster_assignments)]
}

data <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)

# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(data) {
  n <- nrow(data)
  precision_sum <- 0
  recall_sum <- 0

  for (i in 1:n) {
    cluster <- data$cluster[i]
    label <- data$label[i]
    
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(data$label[data$cluster == cluster] == label)
    
# Count the total number of items in the same cluster
total_same_cluster <- sum(data$cluster == cluster)
    
# Count the total number of items with the same category
total_same_category <- sum(data$label == label)
    
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
  }

  # Calculate average precision and recall
  precision <- precision_sum / n
  recall <- recall_sum / n

  return(list(precision = precision, recall = recall))
}

# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(data)

# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall

# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")

```
According to the BCubed Precision score of 0.796927 , there is an average of about 79.70% similarity or relatedness between the instances that were assigned to the same cluster. This implies that the algorithm shows good levels of accuracy in identifying instances that are similar within clusters.

On the other hand, the BCubed Recall score of 0.2648996  indicates that, within each genuine cluster, only approximately 26.5% of similar or related instances are correctly allocated to the same cluster on average. This suggests that the recall inside clusters is poor.










